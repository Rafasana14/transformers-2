{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bca44f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:26.431958Z",
     "iopub.status.busy": "2022-06-26T19:22:26.430891Z",
     "iopub.status.idle": "2022-06-26T19:22:45.756727Z",
     "shell.execute_reply": "2022-06-26T19:22:45.755790Z"
    },
    "papermill": {
     "duration": 19.335424,
     "end_time": "2022-06-26T19:22:45.758966",
     "exception": false,
     "start_time": "2022-06-26T19:22:26.423542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\r\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (4.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (2.27.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm) (2022.5.18.1)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.5.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,Dataset, sampler, random_split\n",
    "from torchvision import models\n",
    "!pip install timm # kaggle doesnt have it installed by default\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import glob \n",
    "import math\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771629ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.772027Z",
     "iopub.status.busy": "2022-06-26T19:22:45.771175Z",
     "iopub.status.idle": "2022-06-26T19:22:45.776742Z",
     "shell.execute_reply": "2022-06-26T19:22:45.776073Z"
    },
    "papermill": {
     "duration": 0.013946,
     "end_time": "2022-06-26T19:22:45.778577",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.764631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Método para obtener todas las especies de pájaros a partir de la estructura de carpetas\n",
    "def get_classes(data_dir):\n",
    "    all_data = datasets.ImageFolder(data_dir)\n",
    "    return all_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031913dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.790045Z",
     "iopub.status.busy": "2022-06-26T19:22:45.789790Z",
     "iopub.status.idle": "2022-06-26T19:22:45.795428Z",
     "shell.execute_reply": "2022-06-26T19:22:45.794570Z"
    },
    "papermill": {
     "duration": 0.013373,
     "end_time": "2022-06-26T19:22:45.797364",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.783991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e89e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.809095Z",
     "iopub.status.busy": "2022-06-26T19:22:45.808837Z",
     "iopub.status.idle": "2022-06-26T19:22:45.814194Z",
     "shell.execute_reply": "2022-06-26T19:22:45.813333Z"
    },
    "papermill": {
     "duration": 0.013553,
     "end_time": "2022-06-26T19:22:45.816101",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.802548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_image(image_path,model):\n",
    "    transform_norm = transforms.ToTensor()\n",
    "   # get normalized image\n",
    "    img_normalized = transform_norm(image_path).float()\n",
    "    img_normalized = img_normalized.unsqueeze(0)\n",
    "   # input = Variable(image_tensor)\n",
    "    img_normalized = img_normalized.to(device)\n",
    "   # print(img_normalized.shape)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output =model(img_normalized)\n",
    "     # print(output)\n",
    "        index = output.data.cpu().numpy().argmax()\n",
    "        class_name = classes[index]\n",
    "        return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b5b61c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.827902Z",
     "iopub.status.busy": "2022-06-26T19:22:45.827665Z",
     "iopub.status.idle": "2022-06-26T19:22:45.846216Z",
     "shell.execute_reply": "2022-06-26T19:22:45.845374Z"
    },
    "papermill": {
     "duration": 0.026751,
     "end_time": "2022-06-26T19:22:45.848194",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.821443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")\n",
    "\n",
    "class ImageFolderCustom(datasets.DatasetFolder):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        setname: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        loader: Callable[[str], Any] = datasets.folder.default_loader,\n",
    "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root,\n",
    "            loader,\n",
    "            IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            is_valid_file=is_valid_file,\n",
    "        )\n",
    "        \n",
    "        classes, class_to_idx = self.find_classes(self.root)\n",
    "        self.samples = self.make_dataset2(self.root,setname, class_to_idx, IMG_EXTENSIONS, is_valid_file)\n",
    "        self.imgs = self.samples\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_dataset2(\n",
    "        directory: str,\n",
    "        setname: str,\n",
    "        class_to_idx: Optional[Dict[str, int]] = None,\n",
    "        extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n",
    "        is_valid_file: Optional[Callable[[str], bool]] = None,    \n",
    "    ) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Generates a list of samples of a form (path_to_sample, class).\n",
    "\n",
    "        See :class:`DatasetFolder` for details.\n",
    "\n",
    "        Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function\n",
    "        by default.\n",
    "        \"\"\"\n",
    "        setname = setname\n",
    "        assert setname in ['train','val']\n",
    "\n",
    "        if class_to_idx is None:\n",
    "            _, class_to_idx = find_classes(directory)\n",
    "        elif not class_to_idx:\n",
    "            raise ValueError(\"'class_to_index' must have at least one entry to collect any samples.\")\n",
    "\n",
    "        both_none = extensions is None and is_valid_file is None\n",
    "        both_something = extensions is not None and is_valid_file is not None\n",
    "        if both_none or both_something:\n",
    "            raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "\n",
    "        if extensions is not None:\n",
    "\n",
    "            def is_valid_file(x: str) -> bool:\n",
    "                return datasets.folder.has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n",
    "\n",
    "        is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
    "\n",
    "        instances = []\n",
    "        available_classes = set()\n",
    "        for target_class in sorted(class_to_idx.keys()):\n",
    "            class_index = class_to_idx[target_class]\n",
    "            target_dir = os.path.join(directory, target_class)\n",
    "            if not os.path.isdir(target_dir):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "                num_images=len(fnames)\n",
    "                num_separator=math.ceil(num_images*0.9)\n",
    "                i=0\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    if(setname=='train' and i>=0 and i<num_separator or (setname=='val' and i>=num_separator and i<num_images)):\n",
    "                        if is_valid_file(path):\n",
    "                            item = path, class_index\n",
    "                            instances.append(item)\n",
    "                            if target_class not in available_classes:\n",
    "                                available_classes.add(target_class)\n",
    "                    i=i+1\n",
    "    \n",
    "        empty_classes = set(class_to_idx.keys()) - available_classes\n",
    "        if empty_classes:\n",
    "            msg = f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n",
    "            if extensions is not None:\n",
    "                msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n",
    "            raise FileNotFoundError(msg)\n",
    "    \n",
    "        return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008ec3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.859617Z",
     "iopub.status.busy": "2022-06-26T19:22:45.859376Z",
     "iopub.status.idle": "2022-06-26T19:22:45.866522Z",
     "shell.execute_reply": "2022-06-26T19:22:45.865827Z"
    },
    "papermill": {
     "duration": 0.014668,
     "end_time": "2022-06-26T19:22:45.868117",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.853449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70429cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.880241Z",
     "iopub.status.busy": "2022-06-26T19:22:45.879546Z",
     "iopub.status.idle": "2022-06-26T19:22:45.890667Z",
     "shell.execute_reply": "2022-06-26T19:22:45.890014Z"
    },
    "papermill": {
     "duration": 0.019026,
     "end_time": "2022-06-26T19:22:45.892306",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.873280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        #print(\"Input: \", x.shape)\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        #print(\"Patches: \", x.shape)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "        #print(\"Linear1: \", x.shape)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        #print(\"Classification Token: \", x.shape)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "       # print(\"Positional Embedding: \", x.shape)\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93623d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.904490Z",
     "iopub.status.busy": "2022-06-26T19:22:45.904202Z",
     "iopub.status.idle": "2022-06-26T19:22:45.915708Z",
     "shell.execute_reply": "2022-06-26T19:22:45.914965Z"
    },
    "papermill": {
     "duration": 0.019617,
     "end_time": "2022-06-26T19:22:45.917337",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.897720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=5):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train':\n",
    "                model.train() # model to training mode\n",
    "            else:\n",
    "                model.eval() # model to evaluate\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'): \n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1) \n",
    "                    preds = torch.argmax(outputs, 1) # estas dos líneas son lo mismo\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step() # step at end of epoch\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since # slight error\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5947d52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:45.930043Z",
     "iopub.status.busy": "2022-06-26T19:22:45.928798Z",
     "iopub.status.idle": "2022-06-26T19:22:53.889875Z",
     "shell.execute_reply": "2022-06-26T19:22:53.888897Z"
    },
    "papermill": {
     "duration": 7.969262,
     "end_time": "2022-06-26T19:22:53.891893",
     "exception": false,
     "start_time": "2022-06-26T19:22:45.922631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52714 images for training with 400 classes\n",
      "Found 5674 images for validation with 400 classes\n",
      "Found 58388 images for final training with 400 classes\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "    \n",
    "train_data = ImageFolderCustom(root='../input/iais22-birds/birds/birds',setname='train', transform = transform_train)\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=4)\n",
    "train_data_len = len(train_data)\n",
    "\n",
    "val_data = ImageFolderCustom(root='../input/iais22-birds/birds/birds',setname='val', transform = transform_val)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "valid_data_len = len(val_data)\n",
    "\n",
    "print(f\"Found {len(train_data)} images for training with {len(train_data.classes)} classes\")\n",
    "print(f\"Found {len(val_data)} images for validation with {len(val_data.classes)} classes\")\n",
    "\n",
    "final_train_data = datasets.ImageFolder(root='../input/iais22-birds/birds/birds', transform = transform_train)\n",
    "final_train_loader = DataLoader(final_train_data, batch_size=256, shuffle=True, num_workers=4)\n",
    "final_train_data_len = len(final_train_data)\n",
    "\n",
    "print(f\"Found {len(final_train_data)} images for final training with {len(final_train_data.classes)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddaa3d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:53.904548Z",
     "iopub.status.busy": "2022-06-26T19:22:53.903836Z",
     "iopub.status.idle": "2022-06-26T19:22:53.909247Z",
     "shell.execute_reply": "2022-06-26T19:22:53.908536Z"
    },
    "papermill": {
     "duration": 0.013343,
     "end_time": "2022-06-26T19:22:53.910849",
     "exception": false,
     "start_time": "2022-06-26T19:22:53.897506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": val_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    \"train\": train_data_len,\n",
    "    \"val\": valid_data_len\n",
    "}\n",
    "\n",
    "final_dataloaders = {\n",
    "    \"train\": final_train_loader,\n",
    "    \"val\": val_loader\n",
    "}\n",
    "final_dataset_sizes = {\n",
    "    \"train\": final_train_data_len,\n",
    "    \"val\": valid_data_len\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9ea84e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:53.922803Z",
     "iopub.status.busy": "2022-06-26T19:22:53.922211Z",
     "iopub.status.idle": "2022-06-26T19:22:54.340311Z",
     "shell.execute_reply": "2022-06-26T19:22:54.339457Z"
    },
    "papermill": {
     "duration": 0.426618,
     "end_time": "2022-06-26T19:22:54.342833",
     "exception": false,
     "start_time": "2022-06-26T19:22:53.916215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = get_classes(\"../input/iais22-birds/birds/birds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3a89d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:54.355988Z",
     "iopub.status.busy": "2022-06-26T19:22:54.355356Z",
     "iopub.status.idle": "2022-06-26T19:22:58.895231Z",
     "shell.execute_reply": "2022-06-26T19:22:58.894358Z"
    },
    "papermill": {
     "duration": 4.549162,
     "end_time": "2022-06-26T19:22:58.897531",
     "exception": false,
     "start_time": "2022-06-26T19:22:54.348369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "''' \n",
    "    embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "    hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "    num_channels - Number of channels of the input (3 for RGB)\n",
    "    num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "    num_layers - Number of layers to use in the Transformer\n",
    "    num_classes - Number of classes to predict\n",
    "    patch_size - Number of pixels that the patches have per dimension\n",
    "    num_patches - Maximum number of patches an image can have\n",
    "    dropout - Amount of dropout to apply in the feed-forward network and\n",
    "            on the input encoding\n",
    "''' \n",
    "\n",
    "model = VisionTransformer(embed_dim = 512, hidden_dim = 1024, num_channels = 3,\n",
    "                           num_heads = 16, num_layers = 5, num_classes = 400, patch_size = 16, num_patches = 196, dropout=0.2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62298721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:58.910605Z",
     "iopub.status.busy": "2022-06-26T19:22:58.910298Z",
     "iopub.status.idle": "2022-06-26T19:22:58.915800Z",
     "shell.execute_reply": "2022-06-26T19:22:58.915057Z"
    },
    "papermill": {
     "duration": 0.013987,
     "end_time": "2022-06-26T19:22:58.917564",
     "exception": false,
     "start_time": "2022-06-26T19:22:58.903577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.11).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b51c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:22:58.929831Z",
     "iopub.status.busy": "2022-06-26T19:22:58.929466Z",
     "iopub.status.idle": "2022-06-26T19:59:11.311553Z",
     "shell.execute_reply": "2022-06-26T19:59:11.308974Z"
    },
    "papermill": {
     "duration": 2172.390653,
     "end_time": "2022-06-26T19:59:11.313794",
     "exception": false,
     "start_time": "2022-06-26T19:22:58.923141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:29<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 5.7163 Acc: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:14<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 4.9591 Acc: 0.0758\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:24<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 4.5663 Acc: 0.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:12<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 4.0257 Acc: 0.2289\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:23<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.8693 Acc: 0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 3.4142 Acc: 0.3673\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:22<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.4164 Acc: 0.3771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.9983 Acc: 0.4773\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:21<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.1091 Acc: 0.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:12<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.7062 Acc: 0.5530\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:21<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.8707 Acc: 0.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.4869 Acc: 0.6181\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:23<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.6667 Acc: 0.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2789 Acc: 0.6664\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:24<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.5117 Acc: 0.6045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.1171 Acc: 0.7168\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:23<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.3703 Acc: 0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:13<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.9885 Acc: 0.7607\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [03:23<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.2304 Acc: 0.6847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:14<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.8718 Acc: 0.7994\n",
      "\n",
      "Training complete in 36m 12s\n",
      "Best Val Acc: 0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, final_dataloaders, final_dataset_sizes, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "547fd1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T19:59:11.696638Z",
     "iopub.status.busy": "2022-06-26T19:59:11.696271Z",
     "iopub.status.idle": "2022-06-26T20:03:28.320218Z",
     "shell.execute_reply": "2022-06-26T20:03:28.319432Z"
    },
    "papermill": {
     "duration": 257.012402,
     "end_time": "2022-06-26T20:03:28.515979",
     "exception": false,
     "start_time": "2022-06-26T19:59:11.503577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8421278</td>\n",
       "      <td>MAGPIE GOOSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9541931</td>\n",
       "      <td>CALIFORNIA CONDOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1820866</td>\n",
       "      <td>FLAME BOWERBIRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12992</td>\n",
       "      <td>CAPE LONGCLAW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12761483</td>\n",
       "      <td>PELICAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id           Category\n",
       "0   8421278       MAGPIE GOOSE\n",
       "1   9541931  CALIFORNIA CONDOR\n",
       "2   1820866    FLAME BOWERBIRD\n",
       "3     12992      CAPE LONGCLAW\n",
       "4  12761483            PELICAN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = []\n",
    "preds_id = []\n",
    "for filename in glob.glob(\"../input/iais22-birds/submission_test/submission_test/*.jpg\"): \n",
    "    im=Image.open(filename)\n",
    "    id = os.path.basename(filename).split(\".\")[0]\n",
    "    image_list.append(im)\n",
    "    preds_id.append(id)\n",
    "\n",
    "index=[]\n",
    "preds = []\n",
    "for f in image_list:\n",
    "    i = image_list.index(f)+1\n",
    "    predict_class = pre_image(f,model)\n",
    "    index.append(i)\n",
    "    preds.append(predict_class)\n",
    "    if(i%500==0):\n",
    "        print(i)\n",
    "\n",
    "submission = pd.DataFrame(\n",
    "    data =np.array([preds_id,preds ]).T, \n",
    "    columns = [\"Id\", \"Category\"]\n",
    ")\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39b7567f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-26T20:03:28.947152Z",
     "iopub.status.busy": "2022-06-26T20:03:28.946810Z",
     "iopub.status.idle": "2022-06-26T20:03:29.033906Z",
     "shell.execute_reply": "2022-06-26T20:03:29.033076Z"
    },
    "papermill": {
     "duration": 0.327663,
     "end_time": "2022-06-26T20:03:29.035907",
     "exception": false,
     "start_time": "2022-06-26T20:03:28.708244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_seleccionado.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2474.504367,
   "end_time": "2022-06-26T20:03:32.843694",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-26T19:22:18.339327",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
