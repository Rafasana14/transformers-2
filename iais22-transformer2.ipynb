{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport torch\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader,Dataset, sampler, random_split\nfrom torchvision import models\n!pip install timm # kaggle doesnt have it installed by default\nimport timm\nfrom timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nfrom tqdm import tqdm\nimport time\nimport copy\nfrom os import listdir\nfrom os.path import isfile, join\nfrom PIL import Image\nimport glob \nimport math\nfrom typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-17T12:05:05.266996Z","iopub.execute_input":"2022-06-17T12:05:05.267668Z","iopub.status.idle":"2022-06-17T12:05:14.63529Z","shell.execute_reply.started":"2022-06-17T12:05:05.267629Z","shell.execute_reply":"2022-06-17T12:05:14.634337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MÃ©todo para obtener todas las especies de pÃ¡jaros a partir de la estructura de carpetas\ndef get_classes(data_dir):\n    all_data = datasets.ImageFolder(data_dir)\n    return all_data.classes","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:14.637166Z","iopub.execute_input":"2022-06-17T12:05:14.637761Z","iopub.status.idle":"2022-06-17T12:05:14.644014Z","shell.execute_reply.started":"2022-06-17T12:05:14.637719Z","shell.execute_reply":"2022-06-17T12:05:14.643158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MÃ©todo para entrenar el modelo\ndef train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=5):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\"*10)\n        \n        for phase in ['train', 'val']: \n            if phase == 'train':\n                model.train() # model to training mode\n            else:\n                model.eval() # model to evaluate\n            \n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1) # used for accuracy\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            if phase == 'train':\n                scheduler.step() # step at end of epoch\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n            \n            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n        print()\n\n    time_elapsed = time.time() - since # slight error\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n    \n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:14.646223Z","iopub.execute_input":"2022-06-17T12:05:14.647543Z","iopub.status.idle":"2022-06-17T12:05:14.66232Z","shell.execute_reply.started":"2022-06-17T12:05:14.647341Z","shell.execute_reply":"2022-06-17T12:05:14.661346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_image(image_path,model):\n    #img = Image.open(image_path)\n    transform_norm = transforms.Compose([transforms.ToTensor(), \n    transforms.Resize((224,224))])\n   # get normalized image\n    img_normalized = transform_norm(image_path).float()\n    img_normalized = img_normalized.unsqueeze(0)\n   # input = Variable(image_tensor)\n    img_normalized = img_normalized.to(device)\n   # print(img_normalized.shape)\n    with torch.no_grad():\n        model.eval()\n        output =model(img_normalized)\n     # print(output)\n        index = output.data.cpu().numpy().argmax()\n        class_name = classes[index]\n        return class_name","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:14.665332Z","iopub.execute_input":"2022-06-17T12:05:14.666014Z","iopub.status.idle":"2022-06-17T12:05:14.679728Z","shell.execute_reply.started":"2022-06-17T12:05:14.665976Z","shell.execute_reply":"2022-06-17T12:05:14.67869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")\n\nclass ImageFolderCustom(datasets.DatasetFolder):\n    \n    def __init__(\n        self,\n        root: str,\n        setname: str,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n        loader: Callable[[str], Any] = datasets.folder.default_loader,\n        is_valid_file: Optional[Callable[[str], bool]] = None,\n    ):\n        super().__init__(\n            root,\n            loader,\n            IMG_EXTENSIONS if is_valid_file is None else None,\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file,\n        )\n        \n        classes, class_to_idx = self.find_classes(self.root)\n        self.samples = self.make_dataset2(self.root,setname, class_to_idx, IMG_EXTENSIONS, is_valid_file)\n        self.imgs = self.samples\n        \n    @staticmethod\n    def make_dataset2(\n        directory: str,\n        setname: str,\n        class_to_idx: Optional[Dict[str, int]] = None,\n        extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n        is_valid_file: Optional[Callable[[str], bool]] = None,    \n    ) -> List[Tuple[str, int]]:\n        \n        setname = setname\n        assert setname in ['train','val']\n\n        if class_to_idx is None:\n            _, class_to_idx = find_classes(directory)\n        elif not class_to_idx:\n            raise ValueError(\"'class_to_index' must have at least one entry to collect any samples.\")\n\n        both_none = extensions is None and is_valid_file is None\n        both_something = extensions is not None and is_valid_file is not None\n        if both_none or both_something:\n            raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n\n        if extensions is not None:\n\n            def is_valid_file(x: str) -> bool:\n                return datasets.folder.has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n\n        is_valid_file = cast(Callable[[str], bool], is_valid_file)\n\n        instances = []\n        available_classes = set()\n        for target_class in sorted(class_to_idx.keys()):\n            class_index = class_to_idx[target_class]\n            target_dir = os.path.join(directory, target_class)\n            if not os.path.isdir(target_dir):\n                continue\n            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n                num_images=len(fnames)\n                num_separator=math.ceil(num_images*0.9)\n                i=0\n                #print(i, num_separator)\n                #print(setname=='train' and i>=0 and i<num_separator)\n                for fname in sorted(fnames):\n                    path = os.path.join(root, fname)\n                    #print(i)\n                    if(setname=='train' and i>=0 and i<num_separator or (setname=='val' and i>=num_separator and i<num_images)):\n                        if is_valid_file(path):\n                            item = path, class_index\n                            #print(item)\n                            instances.append(item)\n                            if target_class not in available_classes:\n                                available_classes.add(target_class)\n                    i=i+1\n    \n        empty_classes = set(class_to_idx.keys()) - available_classes\n        if empty_classes:\n            msg = f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n            if extensions is not None:\n                msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n            raise FileNotFoundError(msg)\n    \n        return instances","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:14.681745Z","iopub.execute_input":"2022-06-17T12:05:14.682238Z","iopub.status.idle":"2022-06-17T12:05:14.704017Z","shell.execute_reply.started":"2022-06-17T12:05:14.682197Z","shell.execute_reply":"2022-06-17T12:05:14.703175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16*53*53,2048)\n        self.fc2 = nn.Linear(2048, 1000)\n        self.fc3 = nn.Linear(1000, 400)\n\n    def forward(self, x):\n        \n        x = self.pool(F.relu(self.conv1(x)))\n        \n        x = self.pool(F.relu(self.conv2(x)))\n        \n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        \n        x = F.relu(self.fc1(x))\n        \n        x = F.relu(self.fc2(x))\n        \n        x = self.fc3(x)\n        \n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n #train\ntransform_train = transforms.Compose([\n    #transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(),\n    #transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n    #transforms.RandomErasing(p=0.2, value='random')\n])\ntransform_val = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n    \ntrain_data = ImageFolderCustom(root='../input/iais22-birds/birds/birds',setname='train', transform = transform_train)\ntrain_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=4)\ntrain_data_len = len(train_data)\n\nval_data = ImageFolderCustom(root='../input/iais22-birds/birds/birds',setname='val', transform = transform_val)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=True, num_workers=4)\nvalid_data_len = len(val_data)\n\nprint(f\"Found {len(train_data)} images for training with {len(train_data.classes)} classes\")\nprint(f\"Found {len(val_data)} images for validation with {len(val_data.classes)} classes\")\n\nfinal_train_data = datasets.ImageFolder(root='../input/iais22-birds/birds/birds', transform = transform_train)\nfinal_train_loader = DataLoader(final_train_data, batch_size=256, shuffle=True, num_workers=4)\nfinal_train_data_len = len(final_train_data)\n\nprint(f\"Found {len(final_train_data)} images for final training with {len(final_train_data.classes)} classes\")","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:14.705099Z","iopub.execute_input":"2022-06-17T12:05:14.705413Z","iopub.status.idle":"2022-06-17T12:05:17.463418Z","shell.execute_reply.started":"2022-06-17T12:05:14.705389Z","shell.execute_reply":"2022-06-17T12:05:17.462548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\ndataset_sizes = {\n    \"train\": train_data_len,\n    \"val\": valid_data_len\n}\n\nfinal_dataloaders = {\n    \"train\": final_train_loader,\n    \"val\": val_loader\n}\nfinal_dataset_sizes = {\n    \"train\": final_train_data_len,\n    \"val\": valid_data_len\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:17.464608Z","iopub.execute_input":"2022-06-17T12:05:17.465215Z","iopub.status.idle":"2022-06-17T12:05:17.48539Z","shell.execute_reply.started":"2022-06-17T12:05:17.465172Z","shell.execute_reply":"2022-06-17T12:05:17.484528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = get_classes(\"../input/iais22-birds/birds/birds\")","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:17.486819Z","iopub.execute_input":"2022-06-17T12:05:17.48717Z","iopub.status.idle":"2022-06-17T12:05:18.003405Z","shell.execute_reply.started":"2022-06-17T12:05:17.48713Z","shell.execute_reply":"2022-06-17T12:05:18.002536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntorch.backends.cudnn.benchmark = True\n\n\nmodel = Net().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:18.004664Z","iopub.execute_input":"2022-06-17T12:05:18.005028Z","iopub.status.idle":"2022-06-17T12:05:18.334934Z","shell.execute_reply.started":"2022-06-17T12:05:18.004992Z","shell.execute_reply":"2022-06-17T12:05:18.334048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.11)\ncriterion = criterion.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:18.337328Z","iopub.execute_input":"2022-06-17T12:05:18.338193Z","iopub.status.idle":"2022-06-17T12:05:18.343921Z","shell.execute_reply.started":"2022-06-17T12:05:18.338153Z","shell.execute_reply":"2022-06-17T12:05:18.343165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:18.344961Z","iopub.execute_input":"2022-06-17T12:05:18.34534Z","iopub.status.idle":"2022-06-17T12:05:18.353438Z","shell.execute_reply.started":"2022-06-17T12:05:18.345304Z","shell.execute_reply":"2022-06-17T12:05:18.352692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders, dataset_sizes)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:18.354452Z","iopub.execute_input":"2022-06-17T12:05:18.357934Z","iopub.status.idle":"2022-06-17T12:05:18.362378Z","shell.execute_reply.started":"2022-06-17T12:05:18.357899Z","shell.execute_reply":"2022-06-17T12:05:18.361517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, final_dataloaders, final_dataset_sizes)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:05:18.364357Z","iopub.execute_input":"2022-06-17T12:05:18.365176Z","iopub.status.idle":"2022-06-17T12:44:34.042933Z","shell.execute_reply.started":"2022-06-17T12:05:18.365138Z","shell.execute_reply":"2022-06-17T12:44:34.041887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimage_list = []\npreds_id = []\nfor filename in glob.glob(\"../input/iais22-birds/submission_test/submission_test/*.jpg\"): \n    im=Image.open(filename)\n    id = os.path.basename(filename).split(\".\")[0]\n    image_list.append(im)\n    preds_id.append(id)\n\nindex=[]\npreds = []\nfor f in image_list:\n    i = image_list.index(f)+1\n    predict_class = pre_image(f,model)\n    index.append(i)\n    preds.append(predict_class)\n    if(i%500==0):\n        print(i)\n\nsubmission = pd.DataFrame(\n    data =np.array([preds_id,preds ]).T, \n    columns = [\"Id\", \"Category\"]\n)\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T12:44:34.045335Z","iopub.execute_input":"2022-06-17T12:44:34.046072Z","iopub.status.idle":"2022-06-17T12:49:05.250612Z","shell.execute_reply.started":"2022-06-17T12:44:34.04602Z","shell.execute_reply":"2022-06-17T12:49:05.249778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}